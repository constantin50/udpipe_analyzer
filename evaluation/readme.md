# Evaluation

As evaluation data I use 105 phrases (primarily questions). The metric is a number of phrases correctly splitted into subjects, objects, predicates, parameters.

![picture](https://sun9-56.userapi.com/impg/ZYxiaO_keCoPjljM0SyiiYItRzoWXcPdICoPLQ/-34wX4BrRZ4.jpg?size=1506x159&quality=96&proxy=1&sign=5e54478e5e995c8a0a959e011f0b72a9&type=album)

For now (30.12.2020) it is 

```
accuracy = 0.943396

```

One can easily get this result using colab notebook 'parser_evaluation' with eval_data.xlsx 
